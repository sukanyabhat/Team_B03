#Begin Time
print(Sys.time())

#Clean environment
closeAllConnections()
rm(list=ls())
####################################################################################################################################################################################3
#Load library
library(forecast)
library(dplyr)
library(data.table)
####################################################################################################################################################################################3

#Read data file 
godaddy = read.table('student_data_20160418.txt',header = TRUE,sep='\t')
godaddy[[1]] = strptime(as.character(godaddy[[1]]), "%m/%d/%Y")

#add code for derived metrics -Pending -1

####################################################################################################################################################################################3

#Sampling
godaddy=subset(godaddy,godaddy[[1]]>"2012-12-31")

#Total flags
godaddy$tot_row_flag=ifelse((godaddy[[2]]=='Total' | godaddy[[3]]=='Total' | godaddy[[4]]=='Total'),1,0)
godaddy$tot_1_flag=ifelse((godaddy[[2]]=='Total' & godaddy[[3]]!='Total' & godaddy[[4]]!='Total'),1,0)
godaddy$tot_2_flag=ifelse((godaddy[[2]]!='Total' & godaddy[[3]]=='Total' & godaddy[[4]]!='Total'),1,0)
godaddy$tot_3_flag=ifelse((godaddy[[2]]!='Total' & godaddy[[3]]!='Total' & godaddy[[4]]=='Total'),1,0)
godaddy$tot_1_2_flag=ifelse((godaddy[[2]]=='Total' & godaddy[[3]]=='Total' & godaddy[[4]]!='Total'),1,0)
godaddy$tot_2_3_flag=ifelse((godaddy[[2]]!='Total' & godaddy[[3]]=='Total' & godaddy[[4]]=='Total'),1,0)
godaddy$tot_1_3_flag=ifelse((godaddy[[2]]=='Total' & godaddy[[3]]!='Total' & godaddy[[4]]=='Total'),1,0)
godaddy$tot_1_2_3_flag=ifelse((godaddy[[2]]=='Total' & godaddy[[3]]=='Total' & godaddy[[4]]=='Total'),1,0)

#Seperate out the datasets based on where the total flag is present
godaddy_set_1=subset(godaddy,godaddy$tot_row_flag==0,select = c(1:(ncol(godaddy)-8)))
#godaddy_set_2=subset(godaddy,godaddy$tot_1_flag==1,select = c(1,3:(ncol(godaddy)-8))) 
godaddy_set_3=subset(godaddy,godaddy$tot_2_flag==1,select = c(1,2,4:(ncol(godaddy)-8)))
godaddy_set_4=subset(godaddy,godaddy$tot_3_flag==1,select = c(1:3,5:(ncol(godaddy)-8))) 
godaddy_set_5=subset(godaddy,godaddy$tot_1_2_flag==1,select = c(1,4:(ncol(godaddy)-8))) 
#godaddy_set_6=subset(godaddy,godaddy$tot_2_3_flag==1,select = c(1,2,5:(ncol(godaddy)-8))) 
#godaddy_set_7=subset(godaddy,godaddy$tot_1_3_flag==1,select = c(1,3,5:(ncol(godaddy)-8))) 

#ignore this set for now----need to come back to this --pending 2
godaddy_set_8=subset(godaddy,godaddy$tot_1_2_3_flag==1,select = c(1,5:(ncol(godaddy)-8))) 



#Here we know that column 2,3,4 are dimension columns
sample_dimension_combinations=sample_n(unique(subset(godaddy,godaddy$tot_row_flag==0,select = c(2,3,4))),5)
#sample_sub_combinations_6=unique(subset(sample_dimension_combinations,select = c(1)))
#sample_sub_combinations_7=unique(subset(sample_dimension_combinations,select = c(2)))
sample_sub_combinations_5=unique(subset(sample_dimension_combinations,select = c(3)))
sample_sub_combinations_4=unique(subset(sample_dimension_combinations,select = c(1,2)))
#sample_sub_combinations_2=unique(subset(sample_dimension_combinations,select = c(2,3)))
sample_sub_combinations_3=unique(subset(sample_dimension_combinations,select = c(1,3)))

combination_table<<-list(sample_dimension_combinations,sample_sub_combinations_3,sample_sub_combinations_4,sample_sub_combinations_5)

#sampling again

godaddy_set_1=merge(godaddy_set_1,sample_dimension_combinations)[, (names(godaddy_set_1))]
#godaddy_set_2=merge(godaddy_set_2,sample_sub_combinations_2)[,(names(godaddy_set_2))]
godaddy_set_3=merge(godaddy_set_3,sample_sub_combinations_3)[, (names(godaddy_set_3))]
godaddy_set_4=merge(godaddy_set_4,sample_sub_combinations_4)[, (names(godaddy_set_4))]
godaddy_set_5=merge(godaddy_set_5,sample_sub_combinations_5)[, (names(godaddy_set_5))]
#godaddy_set_6=merge(godaddy_set_6,sample_sub_combinations_6)[, (names(godaddy_set_6))]
#godaddy_set_7=merge(godaddy_set_7,sample_sub_combinations_7)[, (names(godaddy_set_7))]

####################################################################################################################################################################################3

#Setting global variables and Xref table

subset=c("godaddy_set_1","godaddy_set_2","godaddy_set_3","godaddy_set_4","godaddy_set_5","godaddy_set_6","godaddy_set_7","godaddy_set_8")
dimension_table=c("sample_dimension_combinations","sample_sub_combinations_2","sample_sub_combinations_3","sample_sub_combinations_4","sample_sub_combinations_5","sample_sub_combinations_6","sample_sub_combinations_7","sample_sub_combinations_8")
dimension_number=c(ncol(sample_dimension_combinations),2,2,ncol(sample_sub_combinations_4),ncol(sample_sub_combinations_5),1,1,0)


Xref_table <<- data.frame(subset,dimension_table,dimension_number)
Initial_date<<-"2015-09-25"
date_list<<-unique(subset(godaddy,select = c(1)))
####################################################################################################################################################################################3

#to understand variable substitution 
# x <- 42
# eval(parse(text = "x"))
# [1] 42
# 
# x <- 42
# deparse(substitute(x))
# [1] "x"

#filling up missing rows
#temp fix, generally expect the missing dimensions to be filled with 0 values

Add_missing_dates=function(data){

  grp=eval(parse(text = toString(Xref_table$dimension_table[Xref_table$subset==deparse(substitute(data))])))
  master_list=merge(date_list,grp)
  master_list = master_list[ do.call(order, master_list), ]
  subset=eval(parse(text =toString(deparse(substitute(data)))))
  subset=merge(subset,master_list,all.y = TRUE)
  subset[,(ncol(master_list)+1):ncol(subset) ][is.na(subset[,(ncol(master_list)+1):ncol(subset)])]=0
  return(subset)
}          
####################################################################################################################################################################################3

godaddy_set_1=Add_missing_dates(godaddy_set_1)
#godaddy_set_2=Add_missing_dates(godaddy_set_2)
godaddy_set_3=Add_missing_dates(godaddy_set_3)
godaddy_set_4=Add_missing_dates(godaddy_set_4)
godaddy_set_5=Add_missing_dates(godaddy_set_5)
#godaddy_set_6=Add_missing_dates(godaddy_set_6)
#godaddy_set_7=Add_missing_dates(godaddy_set_7)

####################################################################################################################################################################################3

Intial_Prediction_File_Creation=function(data){
  
data_train=subset(data,data[[1]]<Initial_date)
dimension=eval(parse(text = toString(Xref_table$dimension_table[Xref_table$subset==deparse(substitute(data))])))
metric_col_start=ncol(dimension)+1
no_of_metric=ncol(data)-(ncol(dimension)+1)

predictions=NULL
#j loops through metrics
#i loops through each dimension combination, eg. APAC - APAC Tier 2 - Product 5
for (j in 1:no_of_metric){
  for(i in 1:nrow(dimension)){
    time_series_data=merge(data_train,dimension[i,,drop=FALSE])[, names(data_train)]
    time_series_data=subset(time_series_data[order(time_series_data[[1]]),],select=c(metric_col_start+j))
    
    if (mean(time_series_data[[1]]) == 0)
      next
    if (dim(time_series_data)[1] < 30)
      next
    
    #find outliers and the best replacement values
    g=data.frame(tsoutliers(time_series_data[[1]]))
    #replace
    time_series_data[g$index,1]=g$replacements
    #convert to time series , units of 7 days

    #Considering the time series as chunks of 7 points    
    time_series_data=ts(time_series_data,frequency = 7)
    #next day prediction
    pred_arima=forecast(auto.arima(time_series_data,approximation=TRUE),h=1)
    #absolute prediction
    abs_pred=pred_arima$mean[1]
    #lower threshold
    prediction_min=pred_arima$lower[2]
    #upper threshold
    prediction_max=pred_arima$upper[2]
    
    
    #combine with old predictions
    predictions=rbind(predictions,cbind(colnames(data_train)[metric_col_start+j],toString(data_train[nrow(data_train),1]+ as.difftime(1, unit="days")),dimension[i,],prediction_min,abs_pred,prediction_max))
  }
}

write.table(predictions,file = paste("Threshold_Initial_", deparse(substitute(data)), ".csv",sep = ""),row.names = F, sep=",",  col.names=FALSE)
}
####################################################################################################################################################################################3

Intial_Prediction_File_Creation(godaddy_set_1)
#Intial_Prediction_File_Creation(godaddy_set_2)
Intial_Prediction_File_Creation(godaddy_set_3)
Intial_Prediction_File_Creation(godaddy_set_4)
Intial_Prediction_File_Creation(godaddy_set_5)
#Intial_Prediction_File_Creation(godaddy_set_6)
#Intial_Prediction_File_Creation(godaddy_set_7)
#Intial_Prediction_File_Creation(godaddy_set_8)


#########################################################################################################################

#Only difference between this and Anomaly_Detection and Anomaly_DetectionTS is tht this read from the prediction file

Anomaly_Detection=function(data){
anomalies=data[1,]
anomalies$prediction_min=NULL
anomalies$abs_pred=NULL
anomalies$prediction_max=NULL
anomalies$anomaly_metric=NULL
anomalies$anomaly_flag=NULL
anomalies=NULL

dimension_number=Xref_table$dimension_number[Xref_table$subset==deparse(substitute(data))]
#dimension_number=Xref_table$dimension_number[Xref_table$subset=="godaddy_set_3"]


#Read Initial prediction file 

#initial_pred = read.csv(paste("Threshold_Initial_","godaddy_set_3",".csv",sep = ""),header = FALSE)
initial_pred = read.csv(paste("Threshold_Initial_",deparse(substitute(data)),".csv",sep = ""),header = FALSE)
initial_pred=cbind(initial_pred[,1:(ncol(initial_pred)-3)],setNames(initial_pred[,(ncol(initial_pred)-2):ncol(initial_pred)],c("prediction_min","abs_pred","prediction_max")))
initial_pred[[2]] = strptime(as.character(initial_pred[[2]]), "%Y-%m-%d")

for (j in (dimension_number+2):ncol(data)){
#Check metric
  
initial_prediction =subset(initial_pred,initial_pred[[1]]==colnames(data)[j],select=c(2:ncol(initial_pred)))

#get the current date observed values
godaddy_current_data_set=subset(data,data[[1]]==unique(initial_prediction[[1]]))

#join with predictions to check where the metric is less than predicted lower
merged=merge(godaddy_current_data_set,initial_prediction,by.x = c(1:(dimension_number+1)),by.y  = c(1:(dimension_number+1)))

anomalies_new=subset(merged, merged[,j]<merged[,(ncol(merged)-2)])
if(nrow(anomalies_new)>0){
anomalies_new$anomaly_metric=colnames(anomalies_new)[j]
anomalies_new$anomaly_flag=1
anomalies=rbind(anomalies,anomalies_new)}
}
#write.csv(anomalies,file = paste("Anomalies_", deparse(substitute(data)), ".csv",sep = ""),row.names = F)

return(anomalies)
}

##########################################################################################################################################################
anomalies_set_1=Anomaly_Detection(godaddy_set_1)
#anomalies_set_2=Anomaly_Detection(godaddy_set_2)
anomalies_set_3=Anomaly_Detection(godaddy_set_3)
anomalies_set_4=Anomaly_Detection(godaddy_set_4)
anomalies_set_5=Anomaly_Detection(godaddy_set_5)
#anomalies_set_6=Anomaly_Detection(godaddy_set_6)
#anomalies_set_7=Anomaly_Detection(godaddy_set_7)
#anomalies_set_8=Anomaly_Detection(godaddy_set_8)

##############################################################################################################################3

#Prediction function

Prediction=function(data,date){
  
  data_train=subset(data,data[[1]]<=date)
  
  dimension_number=Xref_table$dimension_number[Xref_table$subset==deparse(substitute(data))]
  dimension_table=Xref_table$dimension_table[Xref_table$subset==deparse(substitute(data))]
  
  #dimension_number=Xref_table$dimension_number[Xref_table$subset=="godaddy_set_4"]
  #dimension_table=Xref_table$dimension_table[Xref_table$subset=="godaddy_set_4"]
 
  metric_col_start=dimension_number+1
  no_of_metric=ncol(data)-(dimension_number+1)
  dimension=data.frame(eval(parse(text = toString(Xref_table$dimension_table[Xref_table$subset==deparse(substitute(data))]))))
 
  predictions=NULL
  #j loops through metrics
  # i loops through dimension groups
  for (j in 1:no_of_metric){
    for(i in 1:nrow(dimension)){
      time_series_data=merge(data_train,dimension[i,,drop=FALSE])[, names(data_train)]
       time_series_data=subset(time_series_data[order(time_series_data[[1]]),],select=c(metric_col_start+j))
      
      if (mean(time_series_data[[1]]) == 0)
        next
      if (dim(time_series_data)[1] < 30)
        next
      #need to think if this helps or not
      
      #find outliers and the best replacement values
      g=data.frame(tsoutliers(time_series_data[[1]]))
      #replace
      time_series_data[g$index,1]=g$replacements
      #convert to time series , units of 7 days
      
      time_series_data=ts(time_series_data,frequency = 7)
      #next day prediction
      pred_arima=forecast(auto.arima(time_series_data,approximation=TRUE),h=1)
      #absolute prediction
      abs_pred=pred_arima$mean[1]
      #lower threshold
      prediction_min=pred_arima$lower[2]
      #upper threshold
      prediction_max=pred_arima$upper[2]
      
      
      #combine with old predictions
      predictions=rbind(predictions,cbind(colnames(data_train)[metric_col_start+j],(date+ as.difftime(1, unit="days")),dimension[i,,drop=FALSE],prediction_min,abs_pred,prediction_max))
    }
  }
  
return(predictions)
}
###########################################################################################################


Anomaly_DetectionTS=function(data,preds){

  dimension_number=Xref_table$dimension_number[Xref_table$subset==deparse(substitute(data))]
  #dimension_number=Xref_table$dimension_number[Xref_table$subset=="godaddy_set_5"]
  anomalies=data[1,]
  anomalies$prediction_min=NULL
  anomalies$abs_pred=NULL
  anomalies$prediction_max=NULL
  anomalies$anomaly_metric=NULL
  anomalies$anomaly_flag=NULL
  anomalies=NULL
  #Read Initial prediction file 
  
  initial_pred = preds
  
  initial_pred[[2]] = strptime(as.character(initial_pred[[2]]), "%Y-%m-%d" )
  
  for (j in (dimension_number+2):ncol(data)){
    #Check metric
    
    initial_prediction =subset(initial_pred,initial_pred[[1]]==colnames(data)[j],select=c(2:ncol(initial_pred)))
    
    #get the current date observed values
    godaddy_current_data_set=subset(data,data[[1]]==unique(initial_prediction[[1]]))
    
    #join with predictions to check where the metric is less than predicted
    merged=merge(godaddy_current_data_set,initial_prediction,by.x = c(1:(dimension_number+1)),by.y  = c(1:(dimension_number+1)))
    
    anomalies_new=subset(merged, merged[,j]<merged[,(ncol(merged)-2)])
    if(nrow(anomalies_new)>0){
      anomalies_new$anomaly_metric=colnames(anomalies_new)[j]
      anomalies_new$anomaly_flag=1
      anomalies=rbind(anomalies,anomalies_new)}
  }
  
  return(anomalies)
}
###################################################################################################

test_date_list=unique(subset(godaddy[order(godaddy[[1]]),],godaddy[[1]]>=Initial_date,select = c(1)))

for (current_date in 1:nrow(test_date_list)){

print(test_date_list[current_date,])
  
predicted_values_1=Prediction(godaddy_set_1,test_date_list[current_date,])
#predicted_values_2=Prediction(godaddy_set_2,test_date_list[current_date,])
predicted_values_3=Prediction(godaddy_set_3,test_date_list[current_date,])
predicted_values_4=Prediction(godaddy_set_4,test_date_list[current_date,])
predicted_values_5=Prediction(godaddy_set_5,test_date_list[current_date,])
#predicted_values_6=Prediction(godaddy_set_6,test_date_list[current_date,])
#predicted_values_7=Prediction(godaddy_set_7,test_date_list[current_date,])
#predicted_values_8=Prediction(godaddy_set_8,test_date_list[current_date,])

anomalies_set_1=rbind(anomalies_set_1,Anomaly_DetectionTS(godaddy_set_1,predicted_values_1))
#anomalies_set_2=rbind(anomalies_set_2,Anomaly_DetectionTS(godaddy_set_2,predicted_values_2))
anomalies_set_3=rbind(anomalies_set_3,Anomaly_DetectionTS(godaddy_set_3,predicted_values_3))
anomalies_set_4=rbind(anomalies_set_4,Anomaly_DetectionTS(godaddy_set_4,predicted_values_4))
anomalies_set_5=rbind(anomalies_set_5,Anomaly_DetectionTS(godaddy_set_5,predicted_values_5))
#anomalies_set_6=rbind(anomalies_set_6,Anomaly_DetectionTS(godaddy_set_6,predicted_values_6))
#anomalies_set_7=rbind(anomalies_set_7,Anomaly_DetectionTS(godaddy_set_7,predicted_values_7))
#anomalies_set_8=rbind(anomalies_set_8,Anomaly_DetectionTS(godaddy_set_8,predicted_values_8))

}

################################################################################################################

#mAKE BELOW repeated set of code as function

if(is.null(anomalies_set_1)==FALSE){
output_1=merge(godaddy_set_1,anomalies_set_1,all.x=TRUE)
output_1$prediction_min[is.na(output_1$prediction_min)==TRUE]=0
output_1$abs_pred[is.na(output_1$abs_pred)==TRUE]=0
output_1$prediction_max[is.na(output_1$prediction_max)==TRUE]=0
output_1$anomaly_metric[is.na(output_1$anomaly_metric)==TRUE]=0
output_1$anomaly_flag[is.na(output_1$anomaly_flag)==TRUE]=0
temp=subset(output_1,anomaly_flag==1)
output_1$deviation[output_1$anomaly_flag==1]=abs(eval(parse(text=paste("temp$",temp[["anomaly_metric"]],sep="")))-temp$prediction_min)
output_1$deviation[output_1$anomaly_flag==0]=0

for(i in 1:nrow(output_1))
{ 
  if(output_1$anomaly_flag[i]==0)
    next
  date = output_1[i,1]
  r1 = output_1[i,2]
  r2 = output_1[i,3]
  id = output_1[i,4]
  w0 = godaddy_set_1[godaddy_set_1[[1]]==date & godaddy_set_1[[2]]==r1 & godaddy_set_1[[3]]==r2 & godaddy_set_1[[4]]==id,5]
  w1 = godaddy_set_1[godaddy_set_1[[1]]==(date- as.difftime(7, unit="days")) & godaddy_set_1[[2]]==r1 & godaddy_set_1[[3]]==r2 & godaddy_set_1[[4]]==id,5]
  w2 = godaddy_set_1[godaddy_set_1[[1]]==(date- as.difftime(14, unit="days")) & godaddy_set_1[[2]]==r1 & godaddy_set_1[[3]]==r2 & godaddy_set_1[[4]]==id,5]
  w3 = godaddy_set_1[godaddy_set_1[[1]]==(date- as.difftime(21, unit="days")) & godaddy_set_1[[2]]==r1 & godaddy_set_1[[3]]==r2 & godaddy_set_1[[4]]==id,5]
  w4 = godaddy_set_1[godaddy_set_1[[1]]==(date- as.difftime(28, unit="days")) & godaddy_set_1[[2]]==r1 & godaddy_set_1[[3]]==r2 & godaddy_set_1[[4]]==id,5]
  w5 = godaddy_set_1[godaddy_set_1[[1]]==(date- as.difftime(35, unit="days")) & godaddy_set_1[[2]]==r1 & godaddy_set_1[[3]]==r2 & godaddy_set_1[[4]]==id,5]
  
  #6 weeks average
  output_1[i,c("average")] = sum(w0,w1,w2,w3,w4,w5,na.rm=TRUE)/ length(c(w0,w1,w2,w3,w4,w5))
  
}
#reverse order rank, largest value smallest rank
output_1$average[output_1$anomaly_flag==0]=0



for(date in 1:nrow(test_date_list))
{

if(nrow(subset(output_1,output_1$anomaly_flag==1 & output_1[[1]]==test_date_list[date,]))  ==0)
  next
  output_1$priority[output_1$anomaly_flag==1 & output_1[[1]]==test_date_list[date,]] = rank(-output_1$average[output_1$anomaly_flag==1 & output_1[[1]]==test_date_list[date,]] ,ties.method ="random")
  }

output_1$priority[output_1$anomaly_flag==0]=0


# #remaining
# deviation column

write.table(output_1,file = paste("Report_", "output_1", ".csv",sep = ""),row.names = F, sep=",")
}
####################################################################################################################

# output_2
###################################################################################################################3

if(is.null(anomalies_set_3)==FALSE){

output_3=merge(godaddy_set_3,anomalies_set_3,all.x=TRUE)
output_3$prediction_min[is.na(output_3$prediction_min)==TRUE]=0
output_3$abs_pred[is.na(output_3$abs_pred)==TRUE]=0
output_3$prediction_max[is.na(output_3$prediction_max)==TRUE]=0
output_3$anomaly_metric[is.na(output_3$anomaly_metric)==TRUE]=0
output_3$anomaly_flag[is.na(output_3$anomaly_flag)==TRUE]=0
temp=subset(output_3,anomaly_flag==1)
output_3$deviation[output_3$anomaly_flag==1]=abs(eval(parse(text=paste("temp$",temp[["anomaly_metric"]],sep="")))-temp$prediction_min)
output_3$deviation[output_3$anomaly_flag==0]=0



for(i in 1:nrow(output_3))
{ 
  if(output_3$anomaly_flag[i]==0)
    next
  date = output_3[i,1]
  r1 = output_3[i,2]
  r2 = output_3[i,3]
  
  w0 = godaddy_set_3[godaddy_set_3[[1]]==date & godaddy_set_3[[2]]==r1 & godaddy_set_3[[3]]==r2 ,4]
  w1 = godaddy_set_3[godaddy_set_3[[1]]==(date- as.difftime(7, unit="days")) & godaddy_set_3[[2]]==r1 & godaddy_set_3[[3]]==r2 ,4]
  w2 = godaddy_set_3[godaddy_set_3[[1]]==(date- as.difftime(14, unit="days")) & godaddy_set_3[[2]]==r1 & godaddy_set_3[[3]]==r2 ,4]
  w3 = godaddy_set_3[godaddy_set_3[[1]]==(date- as.difftime(21, unit="days")) & godaddy_set_3[[2]]==r1 & godaddy_set_3[[3]]==r2 ,4]
  w4 = godaddy_set_3[godaddy_set_3[[1]]==(date- as.difftime(28, unit="days")) & godaddy_set_3[[2]]==r1 & godaddy_set_3[[3]]==r2 ,4]
  w5 = godaddy_set_3[godaddy_set_3[[1]]==(date- as.difftime(35, unit="days")) & godaddy_set_3[[2]]==r1 & godaddy_set_3[[3]]==r2 ,4]
  
  #6 weeks average
  output_3[i,c("average")] = sum(w0,w1,w2,w3,w4,w5,na.rm=TRUE)/ length(c(w0,w1,w2,w3,w4,w5))
  
}
#reverse order rank, largest value smallest rank
output_3$average[output_3$anomaly_flag==0]=0



for(date in 1:nrow(test_date_list))
{
  
  if(nrow(subset(output_3,output_3$anomaly_flag==1 & output_3[[1]]==test_date_list[date,]))  ==0)
    next
  output_3$priority[output_3$anomaly_flag==1 & output_3[[1]]==test_date_list[date,]] = rank(-output_3$average[output_3$anomaly_flag==1 & output_3[[1]]==test_date_list[date,]] ,ties.method ="random")
}

output_3$priority[output_3$anomaly_flag==0]=0

write.table(output_3,file = paste("Report_", "output_3", ".csv",sep = ""),row.names = F, sep=",")
}
##############################################################################################################
if(is.null(anomalies_set_4)==FALSE){
output_4=merge(godaddy_set_4,anomalies_set_4,all.x=TRUE)
output_4$prediction_min[is.na(output_4$prediction_min)==TRUE]=0
output_4$abs_pred[is.na(output_4$abs_pred)==TRUE]=0
output_4$prediction_max[is.na(output_4$prediction_max)==TRUE]=0
output_4$anomaly_metric[is.na(output_4$anomaly_metric)==TRUE]=0
output_4$anomaly_flag[is.na(output_4$anomaly_flag)==TRUE]=0
temp=subset(output_4,anomaly_flag==1)
output_4$deviation[output_4$anomaly_flag==1]=abs(eval(parse(text=paste("temp$",temp[["anomaly_metric"]],sep="")))-temp$prediction_min)
output_4$deviation[output_4$anomaly_flag==0]=0


for(i in 1:nrow(output_4))
{ 

  if(output_4$anomaly_flag[i]==0)
    next
  date = output_4[i,1]
  r1 = output_4[i,2]
  r2 = output_4[i,3]
 
  w0 = godaddy_set_4[godaddy_set_4[[1]]==date & godaddy_set_4[[2]]==r1 & godaddy_set_4[[3]]==r2 ,4]
  w1 = godaddy_set_4[godaddy_set_4[[1]]==(date- as.difftime(7, unit="days")) & godaddy_set_4[[2]]==r1 & godaddy_set_4[[3]]==r2 ,4]
  w2 = godaddy_set_4[godaddy_set_4[[1]]==(date- as.difftime(14, unit="days")) & godaddy_set_4[[2]]==r1 & godaddy_set_4[[3]]==r2 ,4]
  w3 = godaddy_set_4[godaddy_set_4[[1]]==(date- as.difftime(21, unit="days")) & godaddy_set_4[[2]]==r1 & godaddy_set_4[[3]]==r2 ,4]
  w4 = godaddy_set_4[godaddy_set_4[[1]]==(date- as.difftime(28, unit="days")) & godaddy_set_4[[2]]==r1 & godaddy_set_4[[3]]==r2 ,4]
  w5 = godaddy_set_4[godaddy_set_4[[1]]==(date- as.difftime(35, unit="days")) & godaddy_set_4[[2]]==r1 & godaddy_set_4[[3]]==r2 ,4]
  
  #6 weeks average
  output_4[i,c("average")] = sum(w0,w1,w2,w3,w4,w5,na.rm=TRUE)/ length(c(w0,w1,w2,w3,w4,w5))
  
}
#reverse order rank, largest value smallest rank
output_4$average[output_4$anomaly_flag==0]=0


for(date in 1:nrow(test_date_list))
{
  
  if(nrow(subset(output_4,output_4$anomaly_flag==1 & output_4[[1]]==test_date_list[date,]))  ==0)
    next
  output_4$priority[output_4$anomaly_flag==1 & output_4[[1]]==test_date_list[date,]] = rank(-output_4$average[output_4$anomaly_flag==1 & output_4[[1]]==test_date_list[date,]] ,ties.method ="random")
}

output_4$priority[output_4$anomaly_flag==0]=0


write.table(output_4,file = paste("Report_", "output_4", ".csv",sep = ""),row.names = F, sep=",")
}
########################################################################################################################
if(is.null(anomalies_set_5)==FALSE){
  output_5=merge(godaddy_set_5,anomalies_set_5,all.x=TRUE)
  output_5$prediction_min[is.na(output_5$prediction_min)==TRUE]=0
  output_5$abs_pred[is.na(output_5$abs_pred)==TRUE]=0
  output_5$prediction_max[is.na(output_5$prediction_max)==TRUE]=0
  output_5$anomaly_metric[is.na(output_5$anomaly_metric)==TRUE]=0
  output_5$anomaly_flag[is.na(output_5$anomaly_flag)==TRUE]=0
  temp=subset(output_5,anomaly_flag==1)
  output_5$deviation[output_5$anomaly_flag==1]=abs(eval(parse(text=paste("temp$",temp[["anomaly_metric"]],sep="")))-temp$prediction_min)
  output_5$deviation[output_5$anomaly_flag==0]=0
  
  
  
  for(i in 1:nrow(output_5))
  { 
    if(output_5$anomaly_flag[i]==0)
      next
    date = output_5[i,1]
    r1 = output_5[i,2]
   
    w0 = godaddy_set_5[godaddy_set_5[[1]]==date & godaddy_set_5[[2]]==r1  ,3]
    w1 = godaddy_set_5[godaddy_set_5[[1]]==(date- as.difftime(7, unit="days")) & godaddy_set_5[[2]]==r1  ,3]
    w2 = godaddy_set_5[godaddy_set_5[[1]]==(date- as.difftime(14, unit="days")) & godaddy_set_5[[2]]==r1  ,3]
    w3 = godaddy_set_5[godaddy_set_5[[1]]==(date- as.difftime(21, unit="days")) & godaddy_set_5[[2]]==r1  ,3]
    w4 = godaddy_set_5[godaddy_set_5[[1]]==(date- as.difftime(28, unit="days")) & godaddy_set_5[[2]]==r1  ,3]
    w5 = godaddy_set_5[godaddy_set_5[[1]]==(date- as.difftime(35, unit="days")) & godaddy_set_5[[2]]==r1  ,3]
    
    #6 weeks average
    output_5[i,c("average")] = sum(w0,w1,w2,w3,w4,w5,na.rm=TRUE)/ length(c(w0,w1,w2,w3,w4,w5))
    
  }
  #reverse order rank, largest value smallest rank
  output_5$average[output_5$anomaly_flag==0]=0
  
  
  
  for(date in 1:nrow(test_date_list))
  {
    
    if(nrow(subset(output_5,output_5$anomaly_flag==1 & output_5[[1]]==test_date_list[date,]))  ==0)
      next
    output_5$priority[output_5$anomaly_flag==1 & output_5[[1]]==test_date_list[date,]] = rank(-output_5$average[output_5$anomaly_flag==1 & output_5[[1]]==test_date_list[date,]] ,ties.method ="random")
  }
  
  output_5$priority[output_5$anomaly_flag==0]=0

  write.table(output_5,file = paste("Report_", "output_5", ".csv",sep = ""),row.names = F, sep=",")
}


print(Sys.time())



